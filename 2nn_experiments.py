# -*- coding: utf-8 -*-
"""2NN_experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t0_5drhqpmJoCsjF8W-Z-SOvF2zdtVwR
"""

!pip install torch

"""# Basic 2NN centralized"""

from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)

    def forward(self, x):
        # flatten image input
        x = x.view(-1,784)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        output = self.fc3(x)
        return output


def train(args, model, device, train_loader, criterion, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
            if args.dry_run:
                break

def compute_metrics(preds, targets):
    preds = preds.cpu()
    targets = targets.cpu()
    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average='macro')
    acc = accuracy_score(targets, preds)
    cr = classification_report(targets, preds, digits=5, output_dict=True)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'classification_report': cr
    }


def test(model, device, test_loader, criterion):
    model.eval()
    test_loss = 0
    correct = 0

    preds = torch.empty(0).to(device)
    targets = torch.empty(0).to(device)

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            preds = torch.cat((preds, pred), 0)
            targets = torch.cat((targets, target), 0)
            correct += pred.eq(target.view_as(pred)).sum().item()
    compute_metrics(preds, targets)
    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


def main():
    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
    parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=100, metavar='N', ### Changed
                        help='number of epochs to train (default: 14)')
    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',
                        help='learning rate (default: 1.0)') ### Changed
    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
                        help='Learning rate step gamma (default: 0.7)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--dry-run', action='store_true', default=False,
                        help='quickly check a single pass')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=100, metavar='N', ### Changed
                        help='how many batches to wait before logging training status')
    parser.add_argument('--save-model', action='store_true', default=False,
                        help='For Saving the current Model')
    args, unknown = parser.parse_known_args()
    use_cuda = not args.no_cuda and torch.cuda.is_available()

    torch.manual_seed(args.seed)

    device = torch.device("cuda" if use_cuda else "cpu")
    print(device)

    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    if use_cuda:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True}
        train_kwargs.update(cuda_kwargs)
        test_kwargs.update(cuda_kwargs)

    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])
    dataset1 = datasets.MNIST('../data', train=True, download=True,
                       transform=transform)
    dataset2 = datasets.MNIST('../data', train=False,
                       transform=transform)
    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)

    model = Net().to(device)
    optimizer = optim.SGD(model.parameters(), lr=args.lr)
    criterion_train = nn.CrossEntropyLoss()
    criterion_test = nn.CrossEntropyLoss(reduction='sum')

    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, train_loader, criterion_train, optimizer, epoch)
        test(model, device, test_loader, criterion_test)
        #scheduler.step()

    if args.save_model:
        torch.save(model.state_dict(), "mnist_cnn.pt")
main()

"""# Fedavg with N number of clients IID"""

from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import copy
import random
from random import randrange
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
from pathlib import Path
import pickle
import time

class Client:
    def __init__(self, model, criterion, optimizer, dataset):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.dataset = dataset

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)

    def forward(self, x):
        # flatten image input
        x = x.view(-1,784)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        output = F.log_softmax(x, dim=1)
        return output

# Returns an array with N datasets
def split_to_n_datasets(dataset, n):
    dataset_size = len(dataset)//n
    # Seed for reproducible results
    datasets = torch.utils.data.random_split(dataset, [dataset_size for _ in range(n)], generator=torch.Generator().manual_seed(42))
    return datasets

def global_model_to_clients(global_model, clients):
    for client in clients:
        model = client.model
        model.fc1.weight.data = global_model.fc1.weight.data.clone()
        model.fc2.weight.data = global_model.fc2.weight.data.clone()
        model.fc3.weight.data = global_model.fc3.weight.data.clone()
        model.fc1.bias.data = global_model.fc1.bias.data.clone()
        model.fc2.bias.data = global_model.fc2.bias.data.clone()
        model.fc3.bias.data = global_model.fc3.bias.data.clone()
    return clients

# Returns an array with N Client objects.
def create_n_clients(n, global_model, datasets, learning_rate, device):
    clients = []
    criterion_train = nn.CrossEntropyLoss() #move this
    for i in range(n):
        model = Net().to(device)
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
        # the indexes in dataset are the same for client, thus we can do dataset[i].
        client = Client(model, criterion_train, optimizer, datasets[i])
        clients.append(client)
    # Use the same inztialized weights from global model
    global_model_to_clients(global_model, clients)
    return clients

def train_client(args, model, device, train_loader, criterion, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        # if batch_idx % args.log_interval == 0:
        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
        #         epoch, batch_idx * len(data), len(train_loader.dataset),
        #         100. * batch_idx / len(train_loader), loss.item()))
        if args.dry_run:
            break

def models_same(model1, model2):
    for p1, p2 in zip(model1.parameters(), model2.parameters()):
        if p1.data.ne(p2.data).sum() > 0:
            return False
        return True


# Update global model by averaging the clients weights and biases.
# CAUTION: this function assumes clients got equally amount of data!
def update_global_model(global_model, clients, device):
    fc1_mean_weight = torch.zeros(global_model.fc1.weight.shape).to(device)
    fc1_mean_bias = torch.zeros(global_model.fc1.bias.shape).to(device)
    fc2_mean_weight = torch.zeros(global_model.fc2.weight.shape).to(device)
    fc2_mean_bias = torch.zeros(global_model.fc2.bias.shape).to(device)
    fc3_mean_weight = torch.zeros(global_model.fc3.weight.shape).to(device)
    fc3_mean_bias = torch.zeros(global_model.fc3.bias.shape).to(device)
    
    with torch.no_grad():
        for client in clients:
            fc1_mean_weight += client.model.fc1.weight.data.clone()
            fc1_mean_bias += client.model.fc1.bias.data.clone()
            fc2_mean_weight += client.model.fc2.weight.data.clone()
            fc2_mean_bias += client.model.fc2.bias.data.clone()
            fc3_mean_weight += client.model.fc3.weight.data.clone()
            fc3_mean_bias += client.model.fc3.bias.data.clone()

        nr_clients = len(clients)
        fc1_mean_weight =fc1_mean_weight/nr_clients
        fc1_mean_bias = fc1_mean_bias/nr_clients
        fc2_mean_weight = fc2_mean_weight/nr_clients
        fc2_mean_bias = fc2_mean_bias/nr_clients
        fc3_mean_weight =fc3_mean_weight/nr_clients
        fc3_mean_bias = fc3_mean_bias/nr_clients
    
    global_model.fc1.weight.data = fc1_mean_weight
    global_model.fc1.bias.data = fc1_mean_bias
    global_model.fc2.weight.data = fc2_mean_weight
    global_model.fc2.bias.data = fc2_mean_bias
    global_model.fc3.weight.data = fc3_mean_weight
    global_model.fc3.bias.data = fc3_mean_bias
    return global_model

def compute_metrics(preds, targets):
    preds = preds.cpu()
    targets = targets.cpu()
    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average='macro')
    acc = accuracy_score(targets, preds)
    cr = classification_report(targets, preds, digits=5, output_dict=True)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'classification_report': cr
    }

def test(model, device, test_loader, criterion, args):
    model.eval()
    test_loss = 0
    correct = 0

    preds = torch.empty(0).to(device)
    targets = torch.empty(0).to(device)           

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()
            preds = torch.cat((preds, pred), 0)
            targets = torch.cat((targets, target), 0)
            if args.dry_run:
                break

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    
    metrics = compute_metrics(preds, targets)
    metrics["loss"] = test_loss
    
    return metrics

def federated_learning(C):
    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Federated Learning')
    parser.add_argument('--batch-size', type=int, default=10, metavar='N',
        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=100, metavar='N',         ### Changed
        help='number of epochs to train (default: 14)')
    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',
        help='learning rate (default: 1.0)')                                    ### Changed
    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
        help='Learning rate step gamma (default: 0.7)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
        help='disables CUDA training')
    parser.add_argument('--dry-run', action='store_true', default=False,        
        help='quickly check a single pass')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=1000, metavar='N',    ### Changed
        help='how many batches to wait before logging training status')
    parser.add_argument('--save-model', action='store_true', default=True,
        help='For Saving the current Model')
    parser.add_argument('--save-metrics', action='store_true', default=True,
        help='For Saving the client metrics')
    args, unknown = parser.parse_known_args()
    use_cuda = not args.no_cuda and torch.cuda.is_available()

    #For reproducible results
    torch.manual_seed(args.seed)
    random.seed(args.seed)

    device = torch.device("cuda" if use_cuda else "cpu")

    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    if use_cuda:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True}
        train_kwargs.update(cuda_kwargs)
        test_kwargs.update(cuda_kwargs)

    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])
    
    

    ### PARAMETERS
    nr_clients = 100 ### NUMBER OF CLIENTS/ENTITIES
    #C = 100  ### number of clients in each round, [1, nr_clients]
    client_epochs = 1   # Number of client epochs
    communication_rounds = 200  # Number of maximum communication rounds
    test_every_x_round = 1


    dataset = datasets.MNIST('../data', train=True, download=True,
                       transform=transform)
    splitted_datasets = split_to_n_datasets(dataset, nr_clients)
    dataset2 = datasets.MNIST('../data', train=False,
                       transform=transform)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)
    criterion_test = nn.CrossEntropyLoss(reduction='sum')

    global_model = Net().to(device)

    t0 = time.time()

    ### Create n models
    clients = create_n_clients(nr_clients, global_model, splitted_datasets, args.lr, device)

    ### MAIN LOOPS
    metrics_per_test=[]
    for round in range(1, communication_rounds + 1):
        print('Train Round: {}'.format(round))
        for client in random.sample(clients, C):
            
            for client_epoch in range(1, client_epochs + 1):
                train_loader = torch.utils.data.DataLoader(client.dataset,**train_kwargs)
                train_client(args, client.model, device, train_loader, client.criterion, client.optimizer, client_epoch)
        # Update global model
        global_model = update_global_model(global_model, clients, device)
        clients = global_model_to_clients(global_model, clients)
        if round % test_every_x_round == 0:
            metrics_per_test.append(test(global_model, device, test_loader, criterion_test, args))

        if args.dry_run:
            break

    t1 = time.time()
    total_time = t1-t0
    print("Total experimentation time (s): {}".format(total_time))

    
    dir_str = '{}_{}_{}_{}_{}/'.format(nr_clients, C, client_epochs, communication_rounds, test_every_x_round)
    path = '/content/drive/MyDrive/Results/Fedavg/' + dir_str

    if args.save_model:
        Path(path).mkdir(parents=True, exist_ok=True)
        torch.save(global_model.state_dict(), path + "mnist_2nn_fedavg.pt")

    if args.save_metrics:
        metrics = [metrics_per_test, total_time]
        # Directory = (nr_clients)_(C)_(client_epochs)_(communication_rounds)_(test_every_x_round)

        #create dir if nonexistent
        Path(path).mkdir(parents=True, exist_ok=True)

        with open(path + 'metrics', 'wb') as metrics_file:
            pickle.dump(metrics, metrics_file)

federated_learning(10)
federated_learning(20)

"""# Decentralized FL"""

from google.colab import drive
drive.mount('/content/drive')

def calc_avg_clients_metrics(clients_metrics):
    avg_acc = 0
    avg_prec = 0
    avg_rec = 0
    avg_fscore = 0
    avg_loss = 0

    for client_metrics in clients_metrics:
        avg_acc += client_metrics["accuracy"]
        avg_prec += client_metrics["precision"]
        avg_rec += client_metrics["recall"]
        avg_fscore += client_metrics["f1"]
        avg_loss += client_metrics["loss"]
    
    nr_clients = len(clients_metrics)
    avg_acc /= nr_clients
    avg_prec /= nr_clients
    avg_rec /= nr_clients
    avg_fscore /= nr_clients
    avg_loss /= nr_clients

    return {
        'average_accuracy': avg_acc,
        'average_precision': avg_prec,
        'average_recall': avg_rec,
        'average_fscore': avg_fscore,
        'average_loss': avg_loss,
    }

from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import copy
import random
from random import randrange
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
from pathlib import Path
import pickle
import time
 
# Step 2


class Client:
    def __init__(self, model, criterion, optimizer, dataset):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.dataset = dataset

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)

    def forward(self, x):
        # flatten image input
        x = x.view(-1,784)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        output = F.log_softmax(x, dim=1)
        return output

# Returns an array with N datasets
def split_to_n_datasets(dataset, n):
    dataset_size = len(dataset)//n
    # Seed for reproducible results
    datasets = torch.utils.data.random_split(dataset, [dataset_size for _ in range(n)], generator=torch.Generator().manual_seed(42))
    return datasets

def global_model_to_clients(global_model, clients):
    for client in clients:
        model = client.model
        model.fc1.weight.data = global_model.fc1.weight.data.clone()
        model.fc2.weight.data = global_model.fc2.weight.data.clone()
        model.fc3.weight.data = global_model.fc3.weight.data.clone()
        model.fc1.bias.data = global_model.fc1.bias.data.clone()
        model.fc2.bias.data = global_model.fc2.bias.data.clone()
        model.fc3.bias.data = global_model.fc3.bias.data.clone()
    return clients

# Returns an array with N Client objects.
def create_n_clients(n, global_model, datasets, criterion, learning_rate, device):
    clients = []
    for i in range(n):
        model = Net().to(device)
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
        # the indexes in dataset are the same for client, thus we can do dataset[i].
        client = Client(model, criterion, optimizer, datasets[i])
        clients.append(client)
    # Use the same inztialized weights from global model
    global_model_to_clients(global_model, clients)
    return clients

def compute_metrics(preds, targets):
    preds = preds.cpu()
    targets = targets.cpu()
    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average='macro')
    acc = accuracy_score(targets, preds)
    cr = classification_report(targets, preds, digits=5, output_dict=True)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'classification_report': cr
    }

def train_client(args, model, device, train_loader, criterion, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        # if batch_idx % args.log_interval == 0:
        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
        #         epoch, batch_idx * len(data), len(train_loader.dataset),
        #         100. * batch_idx / len(train_loader), loss.item()))
        if args.dry_run:
            break

def models_same(model1, model2):
    for p1, p2 in zip(model1.parameters(), model2.parameters()):
        if p1.data.ne(p2.data).sum() > 0:
            return False
        return True

# Update client models by aggregating and averaging all clients models with neighbors.
# CAUTION: this function assumes clients got equally amount of data!
def update_models(clients, graph, device, C):
    # aggregate and average weight with neighbors
    new_clients = copy.deepcopy(clients)
    for client_idx, client in enumerate(new_clients):
        neighbor_idxs = random.sample(graph[client_idx], C)
        #neighbor_idxs = graph[client_idx]
        for neighbor_idx in neighbor_idxs:
            neighbor = clients[neighbor_idx]

            client.model.fc1.weight.data += neighbor.model.fc1.weight.data.clone()
            client.model.fc1.bias.data += neighbor.model.fc1.bias.data.clone()
            client.model.fc2.weight.data += neighbor.model.fc2.weight.data.clone()
            client.model.fc2.bias.data += neighbor.model.fc2.bias.data.clone()
            client.model.fc3.weight.data += neighbor.model.fc3.weight.data.clone()
            client.model.fc3.bias.data += neighbor.model.fc3.bias.data.clone()

        nr_neighbors = len(neighbor_idxs)+1
        client.model.fc1.weight.data /= nr_neighbors
        client.model.fc1.bias.data /= nr_neighbors
        client.model.fc2.weight.data /= nr_neighbors
        client.model.fc2.bias.data /= nr_neighbors
        client.model.fc3.weight.data /= nr_neighbors
        client.model.fc3.bias.data /= nr_neighbors
    return new_clients

# Run test set on every client 
# Returns: each clients metrics and averages over all clients
def test(clients, device, test_loader, criterion, args):
    clients_metrics = []
    for i, client in enumerate(clients):
        model = client.model
        model.eval()
        test_loss = 0
        correct = 0

        preds = torch.empty(0).to(device)
        targets = torch.empty(0).to(device)

        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                test_loss += criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
                preds = torch.cat((preds, pred), 0)
                targets = torch.cat((targets, target), 0)
                correct += pred.eq(target.view_as(pred)).sum().item()
                if args.dry_run:
                    break

        test_loss /= len(test_loader.dataset)

        metrics = compute_metrics(preds, targets)
        metrics["loss"] = test_loss
        clients_metrics.append(metrics)
    
    return clients_metrics

def decentralized_learning(C):
    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Federated Learning')
    parser.add_argument('--batch-size', type=int, default=10, metavar='N',
        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=100, metavar='N',         ### Changed
        help='number of epochs to train (default: 14)')
    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',
        help='learning rate (default: 1.0)')                                    ### Changed
    parser.add_argument('--no-cuda', action='store_true', default=False,
        help='disables CUDA training')
    parser.add_argument('--dry-run', action='store_true', default=False,        
        help='quickly check a single pass')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
        help='random seed (default: 1)')
    parser.add_argument('--save-models', action='store_true', default=True,
        help='For Saving all client models')
    parser.add_argument('--save-metrics', action='store_true', default=True,
        help='For Saving the client metrics')
    args, unknown = parser.parse_known_args()

    ### PARAMETERS
    nr_clients = 100 # number of clients/entities
    #C = 2  # number of neighbors to communicate with in each round for each clients. range: [0, max_nr_neighbours]
    client_epochs = 1   # Number of client epochs
    communication_rounds = 200  # Number of maximum communication rounds
    test_every_x_round = 10

    graph = [] # A list showing connections, e.g. for client 0 neighbors are at index 0. Unidirectional.
    for i in range(nr_clients):
        neighbors = []
        for j in range(nr_clients):
            if j != i:
                neighbors.append(j)
        graph.append(neighbors)

    # graph = []
    # graph.append([1])
    # for i in range(1, 99):
    #     graph.append([i-1, i+1])
    # graph.append([98])


    #For reproducible results
    torch.manual_seed(args.seed)
    random.seed(args.seed)

    use_cuda = not args.no_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    if use_cuda:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True}
        train_kwargs.update(cuda_kwargs)
        test_kwargs.update(cuda_kwargs)

    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])

    dataset = datasets.MNIST('../data', train=True, download=True,
                       transform=transform)
    splitted_datasets = split_to_n_datasets(dataset, nr_clients)
    dataset2 = datasets.MNIST('../data', train=False,
                       transform=transform)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)

    criterion_test = nn.CrossEntropyLoss(reduction='sum')
    criterion_train = nn.CrossEntropyLoss()

    # All models are initialized with the same weights at start from a "global_model"
    global_model = Net().to(device)

    ### Create n models
    clients = create_n_clients(nr_clients, global_model, splitted_datasets, criterion_train, args.lr, device)

    clients_metrics_per_test = []
    
    ### MAIN LOOPS
    t0 = time.time()
    for round in range(1, communication_rounds+1):
        print('Train Round: {}'.format(round))
        for client in clients:
            for client_epoch in range(1, client_epochs + 1):
                train_loader = torch.utils.data.DataLoader(client.dataset,**train_kwargs)
                train_client(args, client.model, device, train_loader, client.criterion, client.optimizer, client_epoch)
        # Update clients model
        clients = update_models(clients, graph, device, C)

        # Only test every xth communication round due to computationally heavy (time consuming).
        if round % test_every_x_round == 0:
            clients_metrics = test(clients, device, test_loader, criterion_test, args)
            print(calc_avg_clients_metrics(clients_metrics))
            clients_metrics_per_test.append(clients_metrics)

        if args.dry_run:
            break
    
    dir_str = '{}_{}_{}_{}_{}/'.format(nr_clients, C, client_epochs, communication_rounds, test_every_x_round)
    path = '/content/drive/MyDrive/Results/P2P/' + dir_str
    t1 = time.time()
    total_time = t1-t0
    print("Total experimentation time (s): {}".format(total_time))

    if args.save_models:
        #create dirs if nonexistent
        Path(path).mkdir(parents=True, exist_ok=True)
        path_to_models = path + 'models'
        Path(path_to_models).mkdir(parents=True, exist_ok=True)

        for i, client in enumerate(clients):
            torch.save(client.model.state_dict(), "{}/mnist_2nn_client{}.pt".format(path_to_models, i))

    if args.save_metrics:
        metrics = [clients_metrics_per_test, graph, total_time]
        # Directory = (nr_clients)_(C)_(client_epochs)_(communication_rounds)_(test_every_x_round)

        #create dir if nonexistent
        Path(path).mkdir(parents=True, exist_ok=True)

        with open(path + 'metrics', 'wb') as metrics_file:
            pickle.dump(metrics, metrics_file)

decentralized_learning(0)
decentralized_learning(10)
decentralized_learning(20)
decentralized_learning(50)

"""# P2P federated learning

"""

from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import copy
import random
from random import randrange
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
from pathlib import Path
import pickle
import time

transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
    ])

dataset = datasets.MNIST('../data', train=True, download=True,
                      transform=transform)

print(dataset)

"""# Results
Fedavg 100 clients IID,
lr = 0.1,
97% after 30 rounds,

Fedavg 100 clients IID,
lr = 0.1,
C = 0.1,
97 after 272 rounds

----------------

Fedavg 100 clients decentralized
96%~ after 200 epochs

Fedavgdecentralized P2P
lr = 0.1,
C= 0.1,
97% after 32-33 epochs

Fedavgdecentralized P2P
lr = 0.1,
C= 0.05,
97% after 41 epochs

Fedavgdecentralized P2P
lr = 0.1,
C= 0.05,
97% after 60~ epochs

Fedavgdecentralized P2P
lr = 0.1,
C= 0.01,
97% after 100~ epochs
"""



# Returns an array with N datasets
def split_to_n_datasets(dataset, n):
    dataset_size = len(dataset)//n
    # Seed for reproducible results
    datasets = torch.utils.data.random_split(dataset, [dataset_size for _ in range(n)], generator=torch.Generator().manual_seed(42))
    return datasets



#and Non-IID,
#where we first sort the data by digit label, divide it into 200
#shards of size 300, and assign each of 100 clients 2 shards
#

# Creates N shards for every number.
def create_shards(n):
    shards = []
    
    
    for number in range(10):
        dataset = datasets.MNIST('../data', train=True, download=True,
                                 transform=transform)
        idx = dataset.targets==number
        dataset.targets = dataset.targets[idx]
        dataset.data = dataset.data[idx]

        shard_size = len(dataset)//n
        sizes = [shard_size for _ in range(n)]
        rest = len(dataset)-(shard_size*n)
        for i in range(rest):
            idx = i % len(sizes)
            sizes[idx] += 1
        shards.append(torch.utils.data.random_split(dataset, sizes, generator=torch.Generator().manual_seed(42)))
    return shards


# #plit 2 shards into n datasets
def split_shards(shards, n):
    datasets = []
    numbers = [i for i in range(10)]
    for i in range(n):
        if len(numbers) == 1:
            rand_nmbr_1 = numbers[0]
            rand_nmbr_2 = numbers[0]
        else:
            rand_nmbr_1, rand_nmbr_2 = random.sample(numbers, 2)

        shard_1 = shards[rand_nmbr_1].pop()
        shard_2 = shards[rand_nmbr_2].pop()
        if len(shards[rand_nmbr_1]) == 0:
            numbers.remove(rand_nmbr_1)
        if len(shards[rand_nmbr_2]) == 0 and rand_nmbr_2 != rand_nmbr_1:
            numbers.remove(rand_nmbr_2)
        dataset = torch.utils.data.ConcatDataset([shard_1, shard_2])
        datasets.append(dataset)
    return datasets


shards = create_shards(20)

n_datasets = split_shards(shards, 100)
train_loader = torch.utils.data.DataLoader(n_datasets[0], batch_size=10)
for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data, target


n_datasets[0]
#n_datasets[0][400]



df_p2p

#sns.lineplot(x="comm_round", y="average_accuracy", data=df_p2p)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style="darkgrid")

### OPEN P2P
nr_clients = 100 ### NUMBER OF CLIENTS/ENTITIES
C = 1  ### number of clients in each round, [1, nr_clients]
client_epochs = 1   # Number of client epochs
communication_rounds = 200  # Number of maximum communication rounds
test_every_x_round = 10

dir_str = '{}_{}_{}_{}_{}/'.format(nr_clients, C, client_epochs, communication_rounds, test_every_x_round)
path = '/content/drive/MyDrive/Universitet/Thesis/Results/P2P/' + dir_str

with open(path + 'metrics', 'rb') as metrics_file:
    metrics_p2p = pickle.load(metrics_file)

avgs =  [calc_avg_clients_metrics(metrics_p2p[0][i]) for i in range(communication_rounds//test_every_x_round)]
df_p2p = pd.DataFrame(avgs)
comm_round = [i-1 for i in range(10,210,10)]
# 0 indexed
df_p2p['comm_round'] = comm_round
# change columns to match centralized
df_p2p.columns = ['accuracy', 'precision', 'recall', 'f1','loss', 'comm_round']

### OPEN CENTRALIZED
nr_clients = 100 ### NUMBER OF CLIENTS/ENTITIES
C = 100  ### number of clients in each round, [1, nr_clients]
client_epochs = 1   # Number of client epochs
communication_rounds = 200  # Number of maximum communication rounds
test_every_x_round = 1

dir_str = '{}_{}_{}_{}_{}/'.format(nr_clients, C, client_epochs, communication_rounds, test_every_x_round)
path = '/content/drive/MyDrive/Universitet/Thesis/Results/Fedavg/' + dir_str

with open(path + 'metrics', 'rb') as metrics_file:
    metrics = pickle.load(metrics_file)

df = pd.DataFrame.from_dict(metrics[0])
df.drop(columns=["classification_report"])
df["comm_round"] = [i for i in range(200)]

concatenated = pd.concat([df.assign(topology="Centralized"), df_p2p.assign(topology="P2P")])
sns.lineplot(x="comm_round", y="loss", style="topology", data=concatenated)

sns.lineplot(x="comm_round", y="f1", style="topology", data=concatenated)

